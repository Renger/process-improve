function  v = robust_scale(a)
% Using the formula from Mosteller and Tukey, Data Analysis and Regression,
% p 207-208, 1977.
    n = numel(a);
    location = median(a);
    spread_MAD = median(abs(a-location));
    ui = (a - location)/(6*spread_MAD);
    % Valid u_i values used in the summation:
    vu = ui.^2 <= 1;
    num = (a(vu)-location).^2 .* (1-ui(vu).^2).^4;
    den = (1-ui(vu).^2) .* (1-5*ui(vu).^2);
    v = n * sum(num) / (sum(den))^2;
end

#-----
TSR:
https://riunet.upv.es/bitstream/id/303213/PCA%20model%20building%20with%20missing%20data%20new%20proposals%20and%20a%20comparative%20study%20-%20Folch-Fortuny.pdf

#---------
Robust PLS investigated?
    S. Serneels, C. Croux, P. Filzmoser, P.J. Van Espen, Partial Robust M-regression, Chemometrics and Intelligent Laboratory Systems, 79 (2005), 55-64

#------



    def _fit_PLS(self, blockX, blockY):
        """
        Fits a PLS latent variable model between ``blockX`` and ``blockY``.

        1.  HÃ¶skuldsson, PLS regression methods, Journal of Chemometrics,
            2(3), 211-228, 1998, http://dx.doi.org/10.1002/cem.1180020306
        """
        tolerance = np.sqrt(eps)
        itern = 0
        should_terminate = _should_terminate

        X = blockX.data
        Y = blockY.data
        A = blockY.A
        N, K = X.shape
        Ny, M = Y.shape
        if N != Ny:
            raise Warning("The number of observations in both blocks must match.")

        # Initialize storage for certain elements
        blockX.W = np.zeros((blockX.K * blockX.J, blockX.A))  # PLS weights

        # Baseline for all R^2 calcs
        start_SSX_col = blockX.stats.start_SS_col = ssq(X, axis=0)
        start_SSY_col = blockY.stats.start_SS_col = ssq(Y, axis=0)

        if blockY.has_missing:
            if np.any(np.sum(blockY.mmap, axis=1) == 0):
                raise Warning(
                    "Cannot handle the case yet where the entire "
                    "observation in Y-matrix is missing.  Please "
                    "remove those rows and refit model"
                )

        if np.all(start_SSX_col < tolerance):
            raise Exception("There is no variance left in the X-data")
        if np.all(start_SSY_col < tolerance):
            raise Exception("There is no variance left in the Y-data")

        for a in xrange(A):
            start_time = time.time()

            # Initialize t_a with random numbers, or carefully select a column
            # from X
            np.random.seed(0)
            u_a_guess = np.random.uniform(low=-1, high=1, size=(N, 1))
            u_a = u_a_guess + 1.0

            while not (should_terminate(u_a_guess, u_a, itern, tolerance, self)):

                # 0: starting point for convergence checking on next loop
                u_a_guess = u_a.copy()

                # 1: Regress the score, u_a, onto every column in X, compute the
                #    regression coefficient and store in w_a
                # w_a = X.T * u_a / (u_a.T * u_a)
                w_a = regress(X, u_a)

                # 2: Normalize w_a to unit length
                w_a /= np.sqrt(ssq(w_a))

                # 3: Now regress each row in X on the w_a vector, and store the
                #    regression coefficient in t_a
                # t_a = X * w_a / (w_a.T * w_a)
                t_a = regress(X, w_a)

                # 4: Now regress score, t_a, onto every column in Y, compute the
                #    regression coefficient and store in c_a
                # c_a = Y * t_a / (t_a.T * t_a)
                c_a = regress(Y, t_a)

                # 5: Now regress each row in Y on the c_a vector, and store the
                #    regression coefficient in u_a
                # u_a = Y * c_a / (c_a.T * c_a)
                #
                # TODO(KGD):  % Still handle case when entire row in Y is missing
                u_a = regress(Y, c_a)

                itern += 1

            self.stats.timing[a] = time.time() - start_time
            self.stats.itern[a] = itern

            # Loop terminated!
            # 6: Now deflate the X-matrix.  To do that we need to calculate
            # loadings for the X-space.  Regress columns of t_a onto each
            # column in X and calculate loadings, p_a.  Use this p_a to
            # deflate afterwards.
            p_a = regress(X, t_a)  # Note the similarity with step 4!
            X -= np.dot(t_a, p_a.T)  # and that similarity helps understand
            Y -= np.dot(t_a, c_a.T)  # the deflation process.

            # These are the Residual Sums of Squares (RSS); i.e X-X_hat
            row_SSX = ssq(X, axis=1)
            col_SSX = ssq(X, axis=0)
            row_SSY = ssq(Y, axis=1)
            col_SSY = ssq(Y, axis=0)

            blockX.stats.SPE[:, a] = row_SSX / K
            blockX.stats.R2X_k_cum[:, a] = 1 - col_SSX / start_SSX_col
            blockY.stats.R2Y_k_cum[:, a] = 1 - col_SSY / start_SSY_col

            # Cumulative R2 value for the whole block
            blockX.stats.R2[a] = 1 - sum(row_SSX) / sum(start_SSX_col)
            blockY.stats.R2[a] = 1 - sum(row_SSY) / sum(start_SSY_col)

            ## VIP value (only calculated for X-blocks); only last column is useful
            # self.stats.VIP_a = np.zeros((self.K, self.A))
            # self.stats.VIP = np.zeros(self.K)

            # Store results
            # -------------
            # Flip the signs of the column vectors in P so that the largest
            # magnitude element is positive (Wold, Esbensen, Geladi, PCA,
            # CILS, 1987, p 42)
            max_el_idx = np.argmax(np.abs(p_a))
            blockX.W[:, a] = w_a.flatten()
            blockX.P[:, a] = p_a.flatten()
            blockX.T[:, a] = t_a.flatten()
            blockY.C[:, a] = c_a.flatten()
            blockY.U[:, a] = u_a.flatten()
            if np.sign(p_a[max_el_idx]) < 1:
                blockX.W[:, a] *= -1.0
                blockX.P[:, a] *= -1.0
                blockX.T[:, a] *= -1.0
                blockY.C[:, a] *= -1.0
                blockY.U[:, a] *= -1.0

        # end looping on ``a``

        # Calculate Wstar = R = W inv(P'W) or equivalently: (P'W) R' = W'
        blockX.R = np.linalg.solve(np.dot(blockX.P.T, blockX.W), blockX.W.T).T
        blockY.data_pred = np.dot(blockX.T, blockY.C.T)
#-----------
def calc_limits(self):
    """
    Calculate the limits for the latent variable model.

    References
    ----------
    [1]  SPE limits: Nomikos and MacGregor, Multivariate SPC Charts for
            Monitoring Batch Processes. Technometrics, 37, 41-59, 1995.

    [2]  T2 limits: Johnstone and Wischern?
    [3]  Score limits: two methods

            A: Assume that scores are from a two-sided t-distribution with N-1
            degrees of freedom.  Based on the central limit theorem

            B: (t_a/s_a)^2 ~ F_alpha(1, N-1) distribution if scores are
            assumed to be normally distributed, and s_a is chi-squared
            variable with N-1 DOF.

            critical F = scipy.stats.f.ppf(0.95, 1, N-1)
            which happens to be equal to (scipy.stats.t.ppf(0.975, N-1))^2,
            as expected.  Therefore the alpha limit for t_a is equal to
            np.sqrt(scipy.stats.f.ppf(0.95, 1, N-1)) * S[:,a]

            Both methods give the same limits. In fact, some previous code was:
            t_ppf_95 = scipy.stats.t.ppf(0.975, N-1)
            S[:,a] = np.std(this_lv, ddof=0, axis=0)
            lim.t['95.0'][a, :] = t_ppf_95 * S[:,a]

            which assumes the scores were t-distributed.  In fact, the scores
            are not t-distributed, only the (score_a/s_a) is t-distributed, and
            the scores are NORMALLY distributed.
            S[:,a] = np.std(this_lv, ddof=0, axis=0)
            lim.t['95.0'][a, :] = n_ppf_95 * S[:,a]
            lim.t['99.0'][a, :] = n_ppf_99 * [:,a]

            From the CLT: we divide by N, not N-1, but stddev is calculated
            with the N-1 divisor.
    """
    for block in self.blocks:
        N = block.N
        # SPE limits using Nomikos and MacGregor approximation
        for a in xrange(block.A):
            SPE_values = block.stats.SPE[:, a]
            var_SPE = np.var(SPE_values, ddof=1)
            avg_SPE = np.mean(SPE_values)
            chi2_mult = var_SPE / (2.0 * avg_SPE)
            chi2_DOF = (2.0 * avg_SPE ** 2) / var_SPE
            for siglevel_str in block.lim.SPE.keys():
                siglevel = float(siglevel_str) / 100
                block.lim.SPE[siglevel_str][:, a] = chi2_mult * stats.chi2.ppf(
                    siglevel, chi2_DOF
                )

            # For batch blocks: calculate instantaneous SPE using a window
            # of width = 2w+1 (default value for w=2).
            # This allows for (2w+1)*N observations to be used to calculate
            # the SPE limit, instead of just the usual N observations.
            #
            # Also for batch systems:
            # low values of chi2_DOF: large variability of only a few variables
            # high values: more stable periods: all k's contribute

            for siglevel_str in block.lim.T2.keys():
                siglevel = float(siglevel_str) / 100
                mult = (a + 1) * (N - 1) * (N + 1) / (N * (N - (a + 1)))
                limit = stats.f.ppf(siglevel, a + 1, N - (a + 1))
                block.lim.T2[siglevel_str][:, a] = mult * limit

            for siglevel_str in block.lim.t.keys():
                alpha = (1 - float(siglevel_str) / 100.0) / 2.0
                n_ppf = stats.norm.ppf(1 - alpha)
                block.lim.t[siglevel_str][:, a] = n_ppf * block.S[a]
